# poetry install --extras "ui llms-llama-cpp vector-stores-qdrant embeddings-huggingface"


server:
  env_name: ${APP_ENV:local}

###################################################
# llm:
#   mode: llamacpp
#   # Should be matching the selected model



###################################################

llm:
  mode: llamacpp

ollama:
  # llm_model: llama_3.1_q4_km
  llm_model: dorna_q4_ks
  # llm_model: dorna
  # embedding_model: nomic-embed-text
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434  # change if your embedding model runs on another ollama
  keep_alive: 5m
  # keep_alive: '-1'
  
  # tfs_z: 1.0              # Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting.
  # top_k: 40               # Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)
  # top_p: 0.9              # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)
  # repeat_last_n: 64       # Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)
  # repeat_penalty: 1.2     # Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)

  tfs_z: 1.0            # Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting
  top_k: 40             # Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)
  top_p: 0.8           # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)
  repeat_last_n: -1
  repeat_penalty: 1.2   # Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)


  request_timeout: 10000.0  # Time elapsed until ollama times out the request. Default is 120s. Format is float.
  # max_new_tokens: 4096
  context_window: 4096
  temperature: 0.3

openai:
  # api_base: http://172.20.20.252:1234/v1
  api_base: http://127.0.0.1:1234/v1
  api_key: lm-studio
  # model: dorna-llama3-8b-instruct
  model: dorna-llama3-8b-instruct@q4_k_s
  # model: dorna-llama3-8b-instruct
  # model: llama-3.1-8b-lexi-uncensored-v2
  # model: lexi-llama-3-8b-uncensored@q5_k_m
  # model: llama-3.1-8b-instruct-1.2-uncensored
  # model: lmstudio-community/some_models/gemma-2-2b-it-q8_0.gguf
  # model: lmstudio-community/some_models/meta-llama-3.2-3b-instruct-q8_0.gguf
  # max_new_tokens: 4096
  context_window: 4096
  temperature: 0.3

  request_timeout: 10000.0

llamacpp:
  # llm_hf_repo_id: lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF
  llm_hf_model_file: dorna-llama3-8b-instruct.Q8_0.gguf
  # llm_hf_model_file: dorna-llama3-8b-instruct.Q4_K_M.gguf
  # llm_hf_model_file: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
  # llm_hf_model_file: Meta-Llama-3.1-8B-Instruct-Q4_K_M-take2.gguf
  # llm_hf_model_file: dorna-llama3-8b-instruct.Q4_K_S.gguf
  chat_format: llama-3
  # max_new_tokens: 4096
  context_window: 4096
  # tokenizer: meta-llama/Meta-Llama-3.1-8B-Instruct
  prompt_style: "llama3"
  temperature: 0.3
  


vectorstore:
  database: qdrant

qdrant:
  path: local_data/private_gpt/qdrant
###################################################

embedding:
  mode: huggingface
  embed_dim: 1024

huggingface:
  embedding_hf_model_name: jinaai/jina-embeddings-v3
  # embedding_hf_model_name: mixedbread-ai/mxbai-embed-large-v1

vectorstore:
  database: qdrant

qdrant:
  path: local_data/private_gpt/qdrant
